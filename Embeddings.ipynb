{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84b293e5-091a-4194-a296-70cafdfb9f77",
   "metadata": {},
   "source": [
    "# Word and Document Embedding\n",
    "\n",
    "This can be [run in Google Colab](https://colab.research.google.com/github/jreades/ph-word-embeddings/blob/main/Embeddings.ipynb). However, this tutorial was written on a (virtualised) system which had access to 4 CPUs, 20GB of RAM, and a Solid State Disk (SSD) drive. We have included *rough* timings for each step based on those computing resources. You can expect the Colab notebook to be at least 50% slower and it may _crash_ when you get to hierarchical clustering. If that happens to you, we've provided code to subsample the data so that you aren't clustering quite so many observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d091b-235f-4ec4-a53d-1cf255c65336",
   "metadata": {},
   "source": [
    "Generally useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec5590-330c-4c19-92c9-60a1292316ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import random\n",
    "import math\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb40d1-c090-4548-a47c-70e30ab1f3d1",
   "metadata": {},
   "source": [
    "Needed to calculate the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9084ff3e-3a1e-4e73-b552-094abf2fc3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b93fc4-9488-4cd1-a8b6-00c39b916575",
   "metadata": {},
   "source": [
    "Needed for the dimensionality reduction stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9c77c-98c8-45bc-a312-b83773f45825",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "except ModuleNotFoundError:\n",
    "    !pip install umap-learn\n",
    "    import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1f1c0-6dcb-4cf6-b341-3c7e4b02f913",
   "metadata": {},
   "source": [
    "Needed for hierarchical clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c71319-7258-491a-8123-f79801c5505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from kneed import KneeLocator\n",
    "except ModuleNotFoundError:\n",
    "    !pip install kneed\n",
    "    from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f09da9a-f695-43cb-bd32-551b5af55447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, centroid\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from tabulate import tabulate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb337681-ce39-4e6b-840d-46aea442277f",
   "metadata": {},
   "source": [
    "Needed for the validation and visualisation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb98b1-f1f6-4504-a796-f2cba2d54e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a403e-29ff-4814-a46d-0e8e63126048",
   "metadata": {},
   "source": [
    "The code below tries to find a narrow sans-serif TTF font by path that is slightly nicer than the default for the WordCloud library. You would need to update this default for your own system. You can list available fonts using (/ht [imsc](https://stackoverflow.com/a/8755818/4041902)):\n",
    "```python\n",
    "import matplotlib.font_manager\n",
    "matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ebe1da-5b2a-422c-988e-aa0f4ada13f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "fp = fonts[0] # Ensure at least _something_ is set here\n",
    "for f in fonts:\n",
    "    if 'LiberationSansNarrow-Regular' in f:\n",
    "        fp = f\n",
    "        break\n",
    "    elif 'SansNarrow' in f:\n",
    "        fp = f\n",
    "print(f\"Using font: {fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1daa15e-cfa1-44d3-9e1f-91d5979799bf",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db10ef-d3ec-4d69-8362-d3a261101384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "rs = 42\n",
    "\n",
    "# Whether to load a cached model result\n",
    "# to allow further exploration of model\n",
    "# parameters later.\n",
    "cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8636dd-1ea1-4ee0-b3e9-0aec2bc6c070",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a0313-9f6a-4d05-a0d6-28771d228b58",
   "metadata": {},
   "source": [
    "<div style=\"border:dotted 3px red; padding: 15px; background-color: rgb(255,225,225);\">\n",
    "    This cleaned file needs a permanent home that is not my personal site. But also want to ensure BL gets credit for providing the original resource. They might be fine hosting this. Will ask them this week. They can also host the source file, which is a subset of their EThOS resource with DDCs appended.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4109c0dd-6df1-4bd3-93b8-506c4693b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file\n",
    "fn = 'ph-tutorial-data-cleaned.csv.gz'\n",
    "\n",
    "# See if the data has already been downloaded, and\n",
    "# if not, download it from the web site. We save a\n",
    "# copy locally so that you can run this tutorial\n",
    "# offline and also spare the host the bandwidth costs\n",
    "if os.path.exists(os.path.join('data',fn)):\n",
    "    df = pd.read_csv(os.path.join('data',fn), low_memory=False, encoding=\"ISO-8859-1\").set_index('EThOS_ID')\n",
    "else:\n",
    "    # We will look for/create a 'data' directory\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "   \n",
    "    # Download and save\n",
    "    df = pd.read_csv(f'http://www.reades.com/{fn}', low_memory=False, encoding=\"ISO-8859-1\").set_index('EThOS_ID')\n",
    "    df.to_csv(os.path.join('data',fn), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca51f7a-0c71-4dc5-a073-3f5e03d1e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(ast.literal_eval)\n",
    "df['YearGroup'] = df['Year'].apply(lambda x: str(math.floor(int(x)/10)*10)+'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e3f8a3-8703-4e41-939b-7a9522c830a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771c733-66ed-45e6-bf8f-dbb90dcffec3",
   "metadata": {},
   "source": [
    "See Gensim's [word2vec tutorial](https://radimrehurek.com/gensim/models/word2vec.html) for useful background and information about other configuraiton options for the Word2Vec training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24cbd0-26d0-45d1-951a-d5834d67d4ca",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "These functions derive average (unweighted) word embeddings for a document from its constituent words. However, note the possible improvement to the document averaging process from [this page](https://moj-analytical-services.github.io/NLP-guidance/NNmodels.html#from-words-to-documents):\n",
    "\n",
    "> The method we have thus far used has been to calculate the IDF scores for each word in the vocabulary, and then embed each document as an average of the vectors for the words within it, weighted by their IDF scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a297189-dfb9-4844-b17e-cf3ac768b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks up the vector for a given word, returning NaN\n",
    "# if the record's not found. So vlkp == vector lookup\n",
    "def vlkp(w:str, m:Word2Vec) -> np.ndarray:\n",
    "    try:\n",
    "        return m.wv[w]\n",
    "    except KeyError:\n",
    "        return np.nan\n",
    "\n",
    "# Derives the average (mean) embedding for the document \n",
    "# based on the retrieved word vectors. So we iterate over\n",
    "# the tokens and use vlkp to lookup the vector.\n",
    "def avg_embedding(t:list, m:Word2Vec):\n",
    "    vecs = [y for y in [vlkp(x, model) for x in t if x != -1] if not np.isnan(y).all()]\n",
    "    if len(vecs)==0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.mean(np.stack(vecs, axis=0), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c8741d-dbe7-47f6-bebe-0c975eccc0da",
   "metadata": {},
   "source": [
    "### Estimate Dimensionality\n",
    "\n",
    "Using the list of tokens, we can get a sense of the vocabulary we're working with using [this approach](https://stackoverflow.com/a/38896116/4041902). Estimating the right number of dimensions for the embedding is a lot trickier, and the only two examples I've found of this offer _very_ different results since they vary between the square and fourth root of the vocabulary size. Google's [TensorFlow developers](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html) recommend $\\sqrt[4]{d}$, while [Wikipedia](https://en.wikipedia.org/wiki/Word2vec#Dimensionality) indicates \"between 100 and 1,000\" (/ht to [Tom Hale](https://stackoverflow.com/a/55412509/4041902)). This appears to be an [area of active research](https://aclanthology.org/D19-1369.pdf) commercially (and see also [this paper](https://aclanthology.org/I17-2006/)), but working this out is far beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7dc01e-6677-4eb0-bcef-adf25d6fa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = (list(set([a for b in df.tokens for a in b])))\n",
    "print(f\"Data set has a vocabulary of {len(vocab):,} words\")\n",
    "print(f\"An upper estimate of necessary vector size is {math.floor(len(vocab)**0.5)} dimensions.\")\n",
    "print(f\"A lower estimate of the vector size is {math.ceil(len(vocab)**0.25)} dimensions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2feb32-44d1-40be-b9d7-ede66e04974a",
   "metadata": {},
   "source": [
    "### Configure Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8368d3-d227-40ff-9dc9-bb530ceb9b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 100\n",
    "print(f\"You've chosen {dims} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a3bcda-f906-487f-b9c9-bd817c00121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5\n",
    "print(f\"You've chosen a window of size {window}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc5958-3135-47f2-8f30-c175222838fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_v_freq  = 0.0005 # Don't keep words appearing less than 0.05% frequency\n",
    "min_v_count = math.ceil(min_v_freq * df.shape[0])\n",
    "print(f\"With a minimum frequency of {min_v_freq} and data set of size {df.shape[0]:,} the minimum vocab frequency is {min_v_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa48bb64-a51f-4828-b489-1dfdf94828a0",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "\n",
    "Expect generating a Word2Vec model to take **up to 15 minutes** (or **25 minutes on Google Colab**). Note that we've got a `cache` option which allows you to reload an existing model (where the model name is dependent on the number of dimensions and the size of the window) rather than have to recalculate it each time. If you are concerned with **100% replicability** then please also note that you need to **change the number of `workers`** from 4 to 1: this is because you cannot guarantee that documents/words will be passed to the modelling process in the _same order_ when using multiple workers in parallel. So the results should be _consistent_ but not _the same_ from run to run with parallel workers. Of course, running with only one worker will also multiple the model run time by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5c2e8-eb1b-4ba7-a6bc-9fa16d02a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(f\"Word2Vec model has dimensionality of {dims} and window of {window}\") # word or doc\n",
    "\n",
    "# m_nm == model name\n",
    "m_nm = os.path.join('data',f\"word2vec-d{dims}-w{window}.model\")\n",
    "\n",
    "if cache==True and os.path.isfile(m_nm):\n",
    "    # If we're using the cache and the file exists...\n",
    "    print(f\"  Loading existing word2vec model...\")\n",
    "    model = Word2Vec.load(m_nm)\n",
    "    print(f\"    Model loaded from {m_nm}\")\n",
    "else:\n",
    "    print(f\"  Creating new word2vec model...\")\n",
    "    print(f\"    Mininum vocabulary frequency is {min_v_count}\")\n",
    "    print(f\"    Window size is {window}\")\n",
    "    print(f\"    Dimensionality is {dims}\")\n",
    "    # Note issues with full reproducibility with multiple workers: https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "    # See: https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
    "    # And: https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models\n",
    "    \n",
    "    # Note that 'iter' and 'size' are being deprecated in favour of 'vector_size' and \n",
    "    # 'epochs', so depending on which version of Gensim you're using these parameter\n",
    "    # names are likely to change and trigger an errors.\n",
    "    try:\n",
    "        model = Word2Vec(sentences=df.tokens.values.tolist(), vector_size=dims, window=window, epochs=200, \n",
    "                         min_count=min_v_count, seed=rs, workers=4)\n",
    "    except TypeError:\n",
    "        model = Word2Vec(sentences=df.tokens.values.tolist(), size=dims, window=window, iter=200, \n",
    "                         min_count=min_v_count, seed=rs, workers=4)\n",
    "    model.save(m_nm)\n",
    "    print(f\"    Model saved to {m_nm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3a1ef-16a0-45ad-b2d2-5d34eada55ff",
   "metadata": {},
   "source": [
    "### Calculate Average Embedding\n",
    "\n",
    "For word2vec we need to apply an averaging calculation to each record. This takes **about 30 seconds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67d933-b155-4b15-b28f-9d5601cd4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "print(f\"Populating df.word_vec field\")\n",
    "df[f'word_vec'] = df.tokens.apply(avg_embedding, args=(model,))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eac1d7-0d1d-4191-806c-47d775d9d3b5",
   "metadata": {},
   "source": [
    "### Quick Example of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78cd56-caa6-4c40-af4c-e2551f6b2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',150)\n",
    "selected_words = ['accelerator','tourism_development','london_stock_exchange','staphylococcus_aureus','national_health_service']\n",
    "\n",
    "topn  = 7  # top-n most similar words\n",
    "words = []\n",
    "v1    = []\n",
    "v2    = []\n",
    "v3    = []\n",
    "sims  = []\n",
    "\n",
    "for w in selected_words:\n",
    "    vector = model.wv[w]  # get numpy vector of a word\n",
    "    #print(f\"Word vector for '{w}' starts: {vector[:5]}...\")\n",
    "    \n",
    "    sim = model.wv.most_similar(w, topn=topn)\n",
    "    #print(f\"Similar words to '{w}' include: {sim}.\")\n",
    "    \n",
    "    words.append(w)\n",
    "    v1.append(vector[0])\n",
    "    v2.append(vector[1])\n",
    "    v3.append(vector[2])\n",
    "    sims.append(\", \".join([x[0] for x in sim]))\n",
    "    \n",
    "vecs = pd.DataFrame({\n",
    "    'Term':words,\n",
    "    'V1':v1, \n",
    "    'V2':v2, \n",
    "    'V3':v3,\n",
    "    f'Top {topn} Similar':sims\n",
    "})\n",
    "\n",
    "vecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116bb5e-f83e-45e1-bb74-800cd0a3ccea",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n",
    "<div style=\"border:dotted 3px red; padding: 15px; background-color: rgb(255,225,225);\">\n",
    "    Look at whether there are simple things can do with WEs to split the tutorial in half. Eg. What is antonym of X. What are synonyms of Y? (Requires PoS). Basic ‘maths’ demo $a + b - c \\approx d$\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348b7493-aefc-45a1-8600-142bd58c645d",
   "metadata": {},
   "source": [
    "### Similar Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3a5e3-579d-42e5-b433-a10a357d24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',150)\n",
    "selected_words = ['einstein','colorectal_cancer','atlantic_salmon',\n",
    "                  'new_keynesian','land_use_change','semi_structured',\n",
    "                  'influenza_virus','north_east_england','built_environment',\n",
    "                  'information_communication_technology','urban_regeneration',\n",
    "                  'gravitational_wave_detector','cultural_heritage','cultural_capital']\n",
    "\n",
    "topn  = 10  # top-n most similar words\n",
    "words = []\n",
    "sims  = []\n",
    "\n",
    "for w in selected_words:\n",
    "    vector = model.wv[w]  # get numpy vector of a word\n",
    "    #print(f\"Word vector for '{w}' starts: {vector[:5]}...\")\n",
    "    \n",
    "    sim = model.wv.most_similar(w, topn=topn)\n",
    "    #print(f\"Similar to '{w}' include these {len(sim)} terms: {sim}.\")\n",
    "    \n",
    "    words.append(w)\n",
    "    sims.append(\", \".join([x[0] for x in sim]))\n",
    "\n",
    "vecs = pd.DataFrame({\n",
    "    'Term':words,\n",
    "    f'Top {topn} Similar':sims\n",
    "})\n",
    "\n",
    "vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a70bbef-3866-439e-8c7b-cfd785eab7d0",
   "metadata": {},
   "source": [
    "### Term Maths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f735b-f76a-4b61-aa08-17b30595edf6",
   "metadata": {},
   "source": [
    "### Intermediate Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3212dedb-dd83-4c57-bbf2-9cf1eb986690",
   "metadata": {},
   "source": [
    "It can be useful to save intermediate outputs so that you've got a record of what has happened and could either restart the process from this checkpoint (though here you'd need to join back to the original data) or investigate the outputs of the above in greater detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909fddc3-db92-42bf-a12e-c102ff5bc613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Title','tokens','word_vec']].reset_index().to_feather(os.path.join('data','ph-tutorial-data-embeddings.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0afff-b080-4fcc-b758-aed1675c993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='tokens', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8f4f42-28d9-487e-9759-24b1ce3660d1",
   "metadata": {},
   "source": [
    "<div style=\"border:dotted 3px red; padding: 15px; background-color: rgb(255,225,225);\">\n",
    "    If we were going to <b>split this into two tutorials</b> then this is where I'd do it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a681d-6d66-4efb-baf4-6226d374d06b",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721319c2-822d-404d-a327-200c6013c238",
   "metadata": {},
   "source": [
    "While I'm confident about the output from UMAP in a _general_ sense, I'm much less certain about the default _distance_ measure (see [sklearn docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)). I wonder if the metric should be `cosine`? [This article](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa) appears to offer some help, but points on to a [longer discussion](https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions) where `manhattan` is argued to be a good representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120c099-b938-4269-b102-b88f35bf664a",
   "metadata": {},
   "source": [
    "UMAP offers a very wide range of distance metrics:\n",
    "\n",
    "- Minkowski style metrics\n",
    "  - euclidean\n",
    "  - manhattan\n",
    "  - chebyshev\n",
    "  - minkowski\n",
    "- Miscellaneous spatial metrics\n",
    "  - canberra\n",
    "  - braycurtis\n",
    "  - haversine\n",
    "- Normalized spatial metrics\n",
    "  - mahalanobis\n",
    "  - wminkowski\n",
    "  - seuclidean\n",
    "- Angular and correlation metrics\n",
    "  - cosine\n",
    "  - correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a523884-7b5d-4574-9a9d-80afb0a7085b",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02085100-7d7e-466c-afeb-8233408334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes that there is a column that contains the \n",
    "# document embedding as an array/list that needs to be \n",
    "# extracted to a new data frame\n",
    "def x_from_df(df:pd.DataFrame, col:str='Embedding') -> pd.DataFrame:\n",
    "    cols = ['E'+str(x) for x in np.arange(0,len(df[col].iloc[0]))]\n",
    "    return pd.DataFrame(df[col].tolist(), columns=cols, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de754821-a604-4afa-affd-c26d6b85ed26",
   "metadata": {},
   "source": [
    "### Configure Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298009e-5d4f-4c3f-85d5-2e5d83a856c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmeasure = 'euclidean'\n",
    "rdims    = 4 # r-dims == Reduced dimensionality\n",
    "print(f\"UMAP dimensionality reduction to {rdims} dimensions with '{dmeasure}' distance measure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c16607-b5c0-470b-9569-d3c2fc32344d",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality\n",
    "\n",
    "Expect this to take **about 1 minute** (or **2.5 minutes on Google Collab**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76eef11-d053-41d9-a200-5833897b1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "X = x_from_df(df, col='word_vec')\n",
    "    \n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=25,\n",
    "    min_dist=0.01,\n",
    "    n_components=rdims,\n",
    "    random_state=rs)\n",
    "    \n",
    "# Basically reduces our feature vectors for each thesis, down to n dimensions\n",
    "X_embedded = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e004d8cc-4724-42f4-aa9e-43436b53a95c",
   "metadata": {},
   "source": [
    "### Merge on to Data\n",
    "\n",
    "This next block turns the output Numpy array into a data frame with one column for each reduced dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbfa2cf-9867-433a-938c-75e259bcc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dict = {}\n",
    "for i in range(0,X_embedded.shape[1]):\n",
    "    embedded_dict[f\"Dim {i+1}\"] = X_embedded[:,i] # D{dimension_num} (Dim 1...Dim n)\n",
    "\n",
    "# dfe == df embedded\n",
    "dfe = pd.DataFrame(embedded_dict, index=df.index)\n",
    "del(embedded_dict)\n",
    "\n",
    "dfe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e3fb4-3beb-417d-a0c1-fc1f25d2ac4c",
   "metadata": {},
   "source": [
    "Merge the projection on to the main data frame so that we can easily explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7607035-a534-4b61-9e4f-da4399047a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected = df.join(dfe).sort_values(by=['ddc1','ddc2'])\n",
    "print(projected.columns.values)\n",
    "projected.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94c38a-3fc2-4da8-b1da-4bec22c9f581",
   "metadata": {},
   "source": [
    "### Plot Embeddings by DDC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce420a-9069-49fb-876f-a227d5ad9915",
   "metadata": {},
   "source": [
    "We plot these for reference, but don't save them as we spend more time tweaking the outputs in notebook 7 so we can do it for all outputs at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4f4c33-0fdc-48bf-a2d7-3df40d572c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,2,figsize=(14,6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue='ddc1', s=5, alpha=0.1, ax=axs[0]);\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('DDC1 Group')\n",
    "axs[0].get_legend().set_title(\"\")\n",
    "\n",
    "sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue='ddc2', s=5, alpha=0.1, ax=axs[1]);\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('DDC2 Group')\n",
    "axs[1].get_legend().set_title(\"\");\n",
    "#plt.savefig(os.path.join('data','DDC_Plot.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc44eb-b858-4360-a5e7-eb8e4ca67a5d",
   "metadata": {},
   "source": [
    "### Intermediate Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339347f-eb73-4ca5-85e1-01d2b020cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.reset_index().to_feather(os.path.join('data','ph-tutorial-data-final.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f007aa33-f159-4e97-9a65-618b2df6a2f4",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89a9581-9545-4bb7-afa3-05035df749e8",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef84b9-4465-43f9-845d-65d7022e55d9",
   "metadata": {},
   "source": [
    "Expect this next stage to take **about 4 minutes** (or **8 minutes on Google Collab**). Using Google Collab I experienced a crash at this point which is why we have the intermediate saves above. You can reload the data from the feather even when the session ran out of RAM and died. If restarting from this point still produces Out-of-Memory errors than you may need to sample the data instead:\n",
    "```python\n",
    "projected = pd.read_feather(os.path.join('data','ph-tutorial-data-final.feather')).set_index('EThOS_ID').sample(frac=0.5)\n",
    "```\n",
    "When you perform the `join` later the unsampled records should fall out naturally though, obviously, your results will begin to differ substantially from the ones presented in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963fb2a-e6fe-4ac6-b6d8-b9504b14b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "Z = linkage(projected[[x for x in projected.columns if x.startswith('Dim ')]], method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7aeff-e713-4e06-bd43-a4822d51c973",
   "metadata": {},
   "source": [
    "### Intermediate Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332acb5e-6a85-4493-9b31-8eaba24ca55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Z, open(os.path.join('data','Z.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cddacee-2a53-4c6e-9a5e-20ef4fce8122",
   "metadata": {},
   "source": [
    "#### Z-Matrix Output\n",
    "\n",
    "`Z` is a $(n-1)$ by 4 matrix. At the $i$-th iteration, clusters with indices $Z[i, 0]$ and $Z[i, 1]$ are combined to form cluster $n+i$. A cluster with an index less than $n$ corresponds to one of the $n$ original observations. The distance between clusters $Z[i, 0]$ and $Z[i, 1]$ is given by $Z[i, 2]$. The fourth value $Z[i, 3]$ represents the number of original observations in the newly formed cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeca98c-9082-4bf5-acda-3c35f93d70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "\n",
    "# Take the 1st, 10000th, '-2000th', and '-1st' observations\n",
    "for i in [0, 10000, -2000, -1]:\n",
    "    r = list(Z[i])\n",
    "    r.insert(0,(i if i >= 0 else len(Z)+i))\n",
    "    table.append(r)\n",
    "    table[-1][1] = int(table[-1][1])\n",
    "    table[-1][2] = int(table[-1][2])\n",
    "    table[-1][4] = int(table[-1][4])\n",
    "\n",
    "display(\n",
    "    tabulate(table, \n",
    "             headers=[\"Iteration\",\"$c_{i}$\",\"$c_{j}$\",\"$d_{ij}$\",\"$\\sum{c_{i},c_{j}}$\"], \n",
    "             floatfmt='0.3f', tablefmt='html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0908cb14-1d9d-4eb0-8f17-69775eafd6d1",
   "metadata": {},
   "source": [
    "### Show Top _n_ Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61f805e-0c1a-4a7d-8670-08d601e47067",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cls = 100 # The number of last clusters to show in the dendogram\n",
    "\n",
    "plt.title(f'Hierarchical Clustering Dendrogram (truncated at {last_cls} clusters)')\n",
    "plt.xlabel('Sample Index (includes count of records in cluster)')\n",
    "plt.ylabel('Distance')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 7)\n",
    "fig.set_dpi(300)\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp', # truncate dendrogram to the last p merged clusters\n",
    "    p=last_cls,            # and set a value for last p merged clusters\n",
    "    show_leaf_counts=True, # if parentheses then this is a count of observations, otherwise an id\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=False, # to get a distribution impression in truncated branches\n",
    ")\n",
    "#plt.savefig(os.path.join('data',f'Dendogram-{c.dmeasure}-{last_cls}.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4afa3b-d7f7-49b1-bc4c-30c7096c0aae",
   "metadata": {},
   "source": [
    "### Silhouette Scoring\n",
    "\n",
    "This process is slow and computationally intensive: it is calculating a silhouette score for every clustering option between `start` and `end` to give you a sense of how the silhouette score evolves with the number of clusters. A falling silhouette score is normal since, the smaller the number of observations in the cluster, the more you're likely to see some badly-bitted observations within a cluster... at least up until the point where you start having very small clusters indeed. What we're going to be looking for is the 'knee' where this process levels out.\n",
    "\n",
    "You're looking at **_over_ 10 seconds per clustering**, IIRC, so for 50 clusters that's **about 10 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5a4b6-c8bc-429a-9bd8-bfb11c716f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "start_cl = 2\n",
    "end_cl   = 50\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "print(\"Scoring cluster levels: \", end=\"\")\n",
    "\n",
    "X_embedded = projected[[x for x in projected if x.startswith('Dim ')]]\n",
    "\n",
    "for i in range(start_cl,end_cl):\n",
    "    print(\".\", end=\"\")\n",
    "    clusterings = fcluster(Z, i, criterion='maxclust')\n",
    "    \n",
    "    # Calculate silhouett average\n",
    "    sil_avg = silhouette_score(X_embedded, clusterings)\n",
    "    \n",
    "    # Append silhouette scores\n",
    "    sil_scores.append(sil_avg)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a8d0f1-77f2-47b3-b7c9-55401da68932",
   "metadata": {},
   "source": [
    "### Scree Plot\n",
    "\n",
    "Using the silhouette scores calculatee above we can now generate a scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c704c7c-3f51-4153-a913-6c59dbfa9fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(10,5));\n",
    "sns.lineplot(x=np.arange(start_cl,start_cl+len(sil_scores)), y=sil_scores, ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_xlabel(\"Number of Clusters\")\n",
    "ax.set_ylabel(\"Average Silhouette Score\")\n",
    "\n",
    "decreasing = True\n",
    "last_sil   = 1.0\n",
    "selected   = 0\n",
    "\n",
    "for i in range(1,len(sil_scores)):\n",
    "    if sil_scores[i] < last_sil and decreasing:\n",
    "        #print(f\"Decreasing {i}: {sil_scores[i]:0.5f}\")\n",
    "        last_sil = sil_scores[i]\n",
    "    elif sil_scores[i] > last_sil and decreasing:\n",
    "        #print(f\"Increasing {i}: {sil_scores[i]:0.5f}\")\n",
    "        last_sil = sil_scores[i]\n",
    "        decreasing = False\n",
    "    elif sil_scores[i] > last_sil and not decreasing:\n",
    "        #print(f\"Increasing {i}: {sil_scores[i]:0.5f}\")\n",
    "        last_sil = sil_scores[i]\n",
    "        selected = i\n",
    "\n",
    "ax.vlines(x=selected+start_cl, ymin=np.array(sil_scores).min(), ymax=np.array(sil_scores).max(), color='red', linestyle='dotted')\n",
    "plt.text(x=selected+start_cl+1, y=np.array(sil_scores).max()/2, s=f'Suggest $c$={selected+start_cl}')\n",
    "\n",
    "plt.suptitle(f\"Scree Plot for Hierarchical Clustering\", fontsize=14);\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-Scree.png'), dpi=150)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c4090-183e-4acb-9d57-bc2c14d18679",
   "metadata": {},
   "source": [
    "### Knee Locator \n",
    "\n",
    "We can eyeball the scree plot, but a good sanity-check is to use the [kneed](https://pypi.org/project/kneed/) utility to automate the process. Depending on how your clusters score this may or may not be helpful: _e.g._ sometimes a relatively small deviation triggers the 'knee' when that is obviously only a 'blip'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77daa244-e3a4-47ec-be41-9debcb3f7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KneeLocator(np.arange(3,3+len(sil_scores)), sil_scores, curve=\"convex\", direction=\"decreasing\")\n",
    "print(f'Suggest c={kn.knee}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04716d00-6d4f-44d0-b507-1e206b04a974",
   "metadata": {},
   "source": [
    "## Investigating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa174fb-25a3-4b16-8339-a87c505320a3",
   "metadata": {},
   "source": [
    "We're now going to investigate the clustering results in greater detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9131a2e-30ee-4e82-b8f1-b12b5590e91f",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181b3006-1982-445b-a3f9-8c72bffb28ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters(src_df:pd.DataFrame, clusterings:np.ndarray, ddc_level:int=1):\n",
    "    \n",
    "    num_clusters = clusterings.max()\n",
    "    \n",
    "    tmp = pd.DataFrame({f'Cluster_{num_clusters}':clusterings}, index=src_df.index)\n",
    "    \n",
    "    joined_df = src_df.join(tmp, how='inner')\n",
    "    \n",
    "    labels = get_dominant_cat(joined_df, clusterings.max(), ddc_level)\n",
    "    joined_df[f'Cluster_Name_{num_clusters}'] = joined_df[f'Cluster_{num_clusters}'].apply(lambda x: labels[x])\n",
    "\n",
    "    return joined_df\n",
    "\n",
    "def plt_silhouette(src_df:pd.DataFrame, clusterings:np.ndarray) -> plt:\n",
    "    \n",
    "    num_clusters = clusterings.max()\n",
    "    sample_silhouette_values = silhouette_samples(src_df, clusterings)\n",
    "    \n",
    "    scale = cm.get_cmap(get_scale_nm(num_clusters)).colors\n",
    "        \n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,7));\n",
    "    y_lower = 10\n",
    "    mx = clusterings.min()\n",
    "\n",
    "    for cl in range(1,clusterings.max()+1):\n",
    "\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them. \n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[clusterings==cl]\n",
    "        ith_cluster_silhouette_values.sort() # Note, returns None!\n",
    "        y_upper = y_lower + ith_cluster_silhouette_values.shape[0]\n",
    "\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=scale[cl],\n",
    "            edgecolor=scale[cl],\n",
    "            alpha=1.0,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        #ax.text(-0.05, y_lower + 0.5 * ith_cluster_silhouette_values.shape[0], str(c))\n",
    "        ax.annotate(f'Cluster {cl}',\n",
    "            xy=(np.min(sample_silhouette_values), y_lower + 0.5 * ith_cluster_silhouette_values.shape[0]), \n",
    "            textcoords='data', horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(f\"Silhouette Plot for {num_clusters} Clusters\", fontsize=14);\n",
    "    ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "\n",
    "    return ax\n",
    "\n",
    "def get_dominant_cat(clustered_df:pd.DataFrame, num_clusters:int, ddc_level:int=1):\n",
    "    labels = {}\n",
    "    struct = {}\n",
    "\n",
    "    # First, work out the dominant group in each cluster\n",
    "    # and note that together with the cluster number --\n",
    "    # this gives us a dict with key==dominant group and \n",
    "    # then one or more cluster numbers from the output\n",
    "    # above.\n",
    "    for cl in range(1,num_clusters+1):\n",
    "    \n",
    "        # Identify the dominant 'domain' (i.e. group by\n",
    "        # DDC description) using the value counts result.\n",
    "        dom     = clustered_df[clustered_df[f'Cluster_{num_clusters}']==cl][f'ddc{ddc_level}'].value_counts().index[0]\n",
    "        print(f\"Cluster {cl} dominated by {dom} theses.\")\n",
    "    \n",
    "        if struct.get(dom) == None:\n",
    "            struct[dom] = []\n",
    "    \n",
    "        struct[dom].append(cl)\n",
    "\n",
    "    # Next, flip this around so that we create useful\n",
    "    # cluster labels for each cluster. Since we can have\n",
    "    # more than one cluster dominated by the same group\n",
    "    # we have to increment them (e.g. History 1, History 2)\n",
    "    for g in struct.keys():\n",
    "        if len(struct[g])==1:\n",
    "            labels[struct[g][0]]=g\n",
    "            #print(f'{g} maps to Cluster {struct[g][0]}')\n",
    "        else:\n",
    "            for s in range(0,len(struct[g])):\n",
    "                labels[struct[g][s]]=f'{g} {s+1}'\n",
    "                #print(f'{g} {s+1} maps to Cluster {struct[g][s]}')\n",
    "    return labels\n",
    "\n",
    "def get_scale_nm(num_clusters:int):\n",
    "    if num_clusters <= 10:\n",
    "        return 'tab10'\n",
    "    elif num_clusters <= 20:\n",
    "        return 'tab20'\n",
    "    else:\n",
    "        print(\"More than 20 clusters, this is hard to render meaningfully!\")\n",
    "        #cmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"indigo\",\"gold\"], gamma=0.5, N=num_clusters)\n",
    "        #cmap = mcolors.ListedColormap.from_list(\"\", [\"indigo\",\"gold\"], gamma=0.5, N=num_clusters)\n",
    "        cmap = cm.get_cmap('Spectral', num_clusters)\n",
    "        return cmap(np.linspace(0,1,cmap.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68d8953-f0d9-40cf-98cf-c3dbb953f82f",
   "metadata": {},
   "source": [
    "### 2 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a835e1-9dbb-4022-97b5-f6bf2b8c69b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "ddc_level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730eae22-1c8d-42bb-a153-dffc30d9c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Plot silhouette\n",
    "ax = plt_silhouette(projected[[x for x in projected.columns if x.startswith('Dim ')]], clusterings)\n",
    "f = ax.get_figure()\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-c{num_clusters}.png'), dpi=150)\n",
    "plt.show();\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Diagnostics\n",
    "print()\n",
    "\n",
    "# Classification report gives a (statistical) sense of power (TP/TN/FP/FN)\n",
    "print(classification_report(clustered_df[f'ddc{ddc_level}'], clustered_df[f'Cluster_Name_{num_clusters}']))\n",
    "\n",
    "# A confusion matrix is basically a cross-tab (without totals, which I think are nice to add)\n",
    "pd.crosstab(columns=clustered_df[f'Cluster_Name_{num_clusters}'], \n",
    "            index=clustered_df[f'ddc{ddc_level}'], \n",
    "            margins=True, margins_name='Total')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bcf855-9aa3-455d-8ea6-f5f07fc91b01",
   "metadata": {},
   "source": [
    "### 4 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3ca05-ae26-422c-8a9e-bc0626e88867",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "ddc_level = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9e85a-39c9-4723-b03e-a835a360c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Plot silhouette\n",
    "ax = plt_silhouette(projected[[x for x in projected.columns if x.startswith('Dim ')]], clusterings)\n",
    "f = ax.get_figure()\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-c{num_clusters}.png'), dpi=150)\n",
    "plt.show();\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Diagnostics\n",
    "print()\n",
    "\n",
    "# Classification report gives a (statistical) sense of power (TP/TN/FP/FN)\n",
    "print(classification_report(clustered_df[f'ddc{ddc_level}'], clustered_df[f'Cluster_Name_{num_clusters}']))\n",
    "\n",
    "# A confusion matrix is basically a cross-tab (without totals, which I think are nice to add)\n",
    "pd.crosstab(columns=clustered_df[f'Cluster_Name_{num_clusters}'], \n",
    "            index=clustered_df[f'ddc{ddc_level}'], \n",
    "            margins=True, margins_name='Total')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1f25f0-68c1-4262-b9fb-224f46355f8e",
   "metadata": {},
   "source": [
    "### Selected _n_ Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad84b49b-bdc9-4995-a9e9-a8f6b62b35f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "ddc_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641a4f0-fe67-4c55-96e6-0278fe04f4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Plot silhouette\n",
    "ax = plt_silhouette(projected[[x for x in projected.columns if x.startswith('Dim ')]], clusterings)\n",
    "f = ax.get_figure()\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-c{num_clusters}.png'), dpi=150)\n",
    "plt.show();\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Diagnostics\n",
    "print()\n",
    "\n",
    "# If you have a situation where multiple clusters have been detected within \n",
    "# a DDC (e.g. Biochemistry 1, 2, 3) then you need to combine these back into\n",
    "# Biochemsitry for the classification and crosstab or you'll get zero errors\n",
    "# because you couldn't have predicted Biochemistry 2 from Biochemsitry...\n",
    "sddc = clustered_df[f'ddc{ddc_level}']\n",
    "scl  = clustered_df[f'Cluster_Name_{num_clusters}']\n",
    "\n",
    "keys = sorted(scl.unique())\n",
    "vals = [re.sub(' \\d+$','',x) for x in keys]\n",
    "mapping = dict(zip(keys, vals))\n",
    "\n",
    "# Classification report gives a (statistical) sense of power (TP/TN/FP/FN)\n",
    "print(classification_report(sddc, scl.map(mapping), zero_division=1))\n",
    "\n",
    "# A confusion matrix is basically a cross-tab (without totals, which I think are nice to add)\n",
    "pd.crosstab(columns=scl.map(mapping), index=sddc, margins=True, margins_name='Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2165a-c412-45bd-af3e-bed750defecf",
   "metadata": {},
   "source": [
    "## Misclassifications\n",
    "\n",
    "Here we are trying to look in more detail at the PhDs that have (potentially!) been 'misclassified' by the experts--our clustering places them in a different group from the one specified by the DDC. Clearly, we'll have some false-positives in here as well, but the point is to examine the degree to which misclassification is both plausible and useful in terms of demonstrating the value of the NLP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a259734-5496-483d-b0e0-eb2df5d30172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes it easier to read the thesis titles in the output\n",
    "pd.set_option('display.max_colwidth',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de77c69-540b-4f25-9a4f-8f134cb82676",
   "metadata": {},
   "source": [
    "### Configure Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e238510-a8b5-4911-b51e-5210145dffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = ['study','examine','analysis','system','use','design','model','data','within']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd81e7-0a40-41a6-b211-f5a7711b9b75",
   "metadata": {},
   "source": [
    "#### Recluster\n",
    "\n",
    "Not needed if the `num_clusters` and `ddc_level` are the same as you ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58053b7-17d1-4f1e-8091-a71d84a74ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "ddc_level    = 2\n",
    "\n",
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6250d-bb42-4236-a8ea-3099c3923ca0",
   "metadata": {},
   "source": [
    "#### Merge with Intermediate Save\n",
    "\n",
    "We need the tokens again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748fe4f-a0d1-4a5b-91c5-dd9b86b16119",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join('data','ph-tutorial-data-embeddings.feather')).set_index('EThOS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff7712-f638-4518-89d1-dddf5e39d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Data Frame (fdf)\n",
    "fdf = df.join(clustered_df, rsuffix='_dupe')\n",
    "fdf.drop(columns=[x for x in fdf.columns if x.endswith('_dupe')], inplace=True)\n",
    "fdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3fa7b-cda7-404e-a314-b669cedc3485",
   "metadata": {},
   "source": [
    "### Find Misclassified Theses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e915c5a-6dd5-49f8-be0b-c821330a14ee",
   "metadata": {},
   "source": [
    "This approach to misclassification works well for level 1 and level 2 of the DDC, but it gets a lot more complex when you're looking at level 3 because there are _so_ many different groups and misclassifications (e.g. Economics vs Financial Economics) that the results become much harder to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340aa957-e5f0-40d2-bd2e-c603561f19c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be based on whatever clustering above you ran *last* (ncls/ddc_level)\n",
    "# misc = mis-classified records\n",
    "misc = fdf[fdf[f'ddc{ddc_level}'] != fdf[f'Cluster_Name_{num_clusters}']]\n",
    "cols = ['Title',f'ddc{ddc_level}',f'Cluster_Name_{num_clusters}']\n",
    "misc.sample(5, random_state=rs)[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82f1c8c-0458-4a7d-9556-fc18475f8608",
   "metadata": {},
   "source": [
    "Below is the distribution of 'misclassified' theses by DDC group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a796a1-8dcd-4778-b7fa-84114de806a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {misc.shape[0]:,} ({(misc.shape[0]/fdf.shape[0])*100:0.1f}%) 'misclassified' theses.\")\n",
    "print()\n",
    "misc.groupby(by=f'ddc{ddc_level}')[f'Cluster_Name_{num_clusters}'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15e297-5a6f-416c-8a7b-7aa63b5bd4ae",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "This approach is less technically sophisticated and robust than the one set out in Maarten Grootendorst's [CTFIDF](https://github.com/MaartenGr/cTFIDF/blob/master/ctfidf.py) module (as developed in [topic modelling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6) and [class-based TF/IDF](https://towardsdatascience.com/creating-a-class-based-tf-idf-with-scikit-learn-caea7b15b858)), but it saves having to install _another_ module and produces output that is easier to align with the needs of the WordCloud library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2c1ad-7609-4c74-a167-79ef163cedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "tfidfs = {}\n",
    "\n",
    "vec = TfidfVectorizer(use_idf=True, ngram_range=(1,1), smooth_idf=True, stop_words=stopw)\n",
    "\n",
    "for d in fdf[f'ddc{ddc_level}'].unique():\n",
    "    \n",
    "    print(f\"Examining {d} DDC\")\n",
    "    tfidfs[d] = []\n",
    "    \n",
    "    # All records classified under this DDC\n",
    "    ddc_df = fdf[fdf[f'ddc{ddc_level}']==d].copy()\n",
    "    \n",
    "    # Those records that are part of this DDC\n",
    "    # but were clustered with *another* group\n",
    "    # going by the dominant class in that cluster.\n",
    "    sub_df = misc[misc[f'ddc{ddc_level}']==d].copy()\n",
    "    \n",
    "    print(f\"  ddc_df: {ddc_df.shape[0]:>7,}\")\n",
    "    print(f\"  sub_df: {sub_df.shape[0]:>7,}\")\n",
    "    print(f\"  remain: {ddc_df[~ddc_df.index.isin(misc.index)].shape[0]:>7,}\")\n",
    "    \n",
    "    print(f\"  {(sub_df.shape[0]/ddc_df.shape[0])*100:0.1f}% of {d} PhDs were clustered in other disciplines.\")\n",
    "    \n",
    "    # This removes the 'Earth Sc. 2', 'Earth Sc. 1' distinction for example.\n",
    "    # You would normally only encounter this working with DDC3.\n",
    "    sub_df.loc[:,'Cluster Name'] = sub_df[f'Cluster_Name_{num_clusters}'].str.replace(\"\\s\\d+$\",\"\",regex=True)\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    # And fit the corpus using IDF\n",
    "    corpus  = ddc_df.tokens.str.join(' ').fillna(' ').values \n",
    "    vec.fit(corpus)\n",
    "    \n",
    "    # One image per DDC Category\n",
    "    f,axes = plt.subplots(1, len(sub_df['Cluster Name'].unique()), figsize=(14,5))\n",
    "    \n",
    "    for i, cl in enumerate(sub_df['Cluster Name'].unique()):\n",
    "        \n",
    "        sub_cdf = sub_df[sub_df['Cluster Name']==cl]\n",
    "        print(f\"  PhDs classified as {cl} ({sub_cdf.shape[0]:,})\")\n",
    "        \n",
    "        tcorpus = vec.transform(sub_cdf.tokens.str.join(' ').fillna(' ').values)\n",
    "        \n",
    "        tfidf   = pd.DataFrame.from_records(tcorpus.toarray(), index=sub_cdf.index, columns=vec.get_feature_names_out())\n",
    "        tfterms = tfidf.T.sum(axis=1)\n",
    "        \n",
    "        tfidfs[d].append(\n",
    "            pd.DataFrame(\n",
    "              {f\"{cl} Term\":  tfterms.sort_values(ascending=False).index, \n",
    "               f\"{cl} Value\": tfterms.sort_values(ascending=False).values}\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #print(tfterms.sort_values(ascending=False).head(5))\n",
    "        #print()\n",
    "        \n",
    "        Cloud = WordCloud(background_color=None, mode='RGBA', relative_scaling=0.5, font_path=fp)\n",
    "        \n",
    "        ax = axes.flatten()[i]\n",
    "        ax.set_title(f\"{d} clustered with {cl} ($n$={sub_cdf.shape[0]:,})\")\n",
    "        ax.imshow(Cloud.generate_from_frequencies(tfterms))\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(os.path.join('images',f\"DDC_Cloud-c{num_clusters}-ddc{d}-tfidf.png\"), dpi=150)\n",
    "    plt.show()\n",
    "        \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811f980-8078-4481-96e2-7694b4ee34ff",
   "metadata": {},
   "source": [
    "### 15-Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59a712c-5207-47a0-98a9-1b6f224dad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "ddc_level    = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4344a489-0995-48a2-ab16-ba17519157e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Load up intermediate save\n",
    "df = pd.read_feather(os.path.join('data','ph-tutorial-data-embeddings.feather')).set_index('EThOS_ID')\n",
    "\n",
    "# Join up with tokens\n",
    "fdf = df.join(clustered_df, rsuffix='_dupe')\n",
    "fdf.drop(columns=[x for x in fdf.columns if x.endswith('_dupe')], inplace=True)\n",
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a37a8-a092-477d-8eb6-32d4f127312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls  = [x for x in fdf[f'Cluster_Name_{num_clusters}'].unique() if isinstance(x,str)] # Catch unexpected unlabelled result\n",
    "print(f\"Found clusters: {', '.join(sorted(cls))}.\")\n",
    "cols = 3\n",
    "rows = math.ceil(len(cls)/cols)\n",
    "\n",
    "vec = TfidfVectorizer(use_idf=True, ngram_range=(1,1), smooth_idf=True, stop_words=stopw)\n",
    "\n",
    "# We only need to calculate this once since \n",
    "# we're comparing to the full corpus\n",
    "idf_df  = fdf\n",
    "corpus  = idf_df.tokens.str.join(' ').fillna(' ').values # Convert tokens back to string\n",
    "vec.fit(corpus) # And fit the corpus using IDF\n",
    "\n",
    "f,axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n",
    "\n",
    "for i,v in enumerate(sorted(cls)):\n",
    "    \n",
    "    # Truncate cluster long name (cl.nm) just in case\n",
    "    if len(v) > 26:\n",
    "        cln = f\"{v[:26]}...\"\n",
    "    else:\n",
    "        cln = v\n",
    "    ax = axs.flatten()[i]\n",
    "    \n",
    "    print(f\"Cluster name is '{cln}'.\")\n",
    "    \n",
    "    # The TF is from the selected cluster\n",
    "    tf_df  = fdf[fdf[f'Cluster_Name_{num_clusters}']==v]\n",
    "    \n",
    "    # Share of corpus\n",
    "    corpus_share = f\"{(tf_df.shape[0]/idf_df.shape[0])*100:0.2f}\"\n",
    "    print(f\"  Cluster contains {tf_df.shape[0]:,} ({corpus_share}% of records)\")\n",
    "    \n",
    "    # Transform the selected cluster documents\n",
    "    tcorpus = vec.transform(tf_df.tokens.str.join(' ').fillna(' ').values)\n",
    "    \n",
    "    # And get the words back out in a format that\n",
    "    # we can use for a word cloud\n",
    "    tfidf   = pd.DataFrame.from_records(tcorpus.toarray(), index=tf_df.index, columns=vec.get_feature_names_out())\n",
    "    tfterms = tfidf.T.sum(axis=1)\n",
    "    \n",
    "    Cloud = WordCloud(background_color=None, mode='RGBA', \n",
    "                      width=int(ax.bbox.width), height=int(ax.bbox.height),\n",
    "                      relative_scaling=0.5, font_path=fp, \n",
    "                      max_words=50, random_state=rs)\n",
    "    \n",
    "    #print(f\"width={int(ax.bbox.width)}, height={int(ax.bbox.height)}\")\n",
    "    \n",
    "    ax.title.set_text(f\"{cln} ({corpus_share}% of docs)\")\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(Cloud.generate_from_frequencies(tfterms))\n",
    "    print(\"  Done.\")\n",
    "\n",
    "#plt.suptitle(f\"Distinctive Words for Clusters\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('data',f\"Word_Cloud-c{num_clusters}-tfidf.png\"), dpi=150)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
