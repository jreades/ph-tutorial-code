{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering and Visualising Documents using Word Embeddings \n",
    "\n",
    "This can be [run in Google Colab](https://colab.research.google.com/github/jreades/ph-word-embeddings/blob/main/Clustering_Word_Embeddings.ipynb). However, this tutorial was written on a (virtualised) system which had access to 4 CPUs, 8GB of RAM, and a Solid State Disk (SSD) drive. We have included *rough* timings for each step based on those computing resources. You can expect the Colab notebook to be at least 50% slower and it may _crash_ when you get to hierarchical clustering. If that happens to you, we've provided code to subsample the data so that you aren't clustering quite so many observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "import random\n",
    "import math\n",
    "import ast\n",
    "import os\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed to calculate the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed for the dimensionality reduction stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import umap\n",
    "except ModuleNotFoundError:\n",
    "    !pip install umap-learn\n",
    "    import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed for hierarchical clustering stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from kneed import KneeLocator\n",
    "except ModuleNotFoundError:\n",
    "    !pip install kneed\n",
    "    from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, centroid\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from tabulate import tabulate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed for the validation and visualisation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below tries to find a narrow sans-serif TTF font by path that is slightly nicer than the default for the WordCloud library. You would need to update this default for your own system. You can list available fonts using (/ht [imsc](https://stackoverflow.com/a/8755818/4041902)):\n",
    "```python\n",
    "import matplotlib.font_manager\n",
    "matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "fonts = matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n",
    "fp = fonts[0] #Â Ensure at least _something_ is set here\n",
    "for f in fonts:\n",
    "    if 'LiberationSansNarrow-Regular' in f:\n",
    "        fp = f\n",
    "        break\n",
    "    elif 'SansNarrow' in f:\n",
    "        fp = f\n",
    "print(f\"Using font: {fp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "rs = 42\n",
    "\n",
    "# Whether to load a cached model result\n",
    "# to allow further exploration of model\n",
    "# parameters later.\n",
    "cache = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:dotted 3px red; padding: 15px; background-color: rgb(255,225,225);\">\n",
    "    This cleaned file needs a permanent home that is not my personal site. But also want to ensure BL gets credit for providing the original resource. They might be fine hosting this. Will ask them this week. They can also host the source file, which is a subset of their EThOS resource with DDCs appended.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the file\n",
    "fn = 'ph-tutorial-data-cleaned.csv.gz'\n",
    "\n",
    "# See if the data has already been downloaded, and\n",
    "# if not, download it from the web site. We save a\n",
    "# copy locally so that you can run this tutorial\n",
    "# offline and also spare the host the bandwidth costs\n",
    "if os.path.exists(os.path.join('data',fn)):\n",
    "    df = pd.read_csv(os.path.join('data',fn), low_memory=False, encoding=\"ISO-8859-1\").set_index('EThOS_ID')\n",
    "else:\n",
    "    # We will look for/create a 'data' directory\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "   \n",
    "    # Download and save\n",
    "    df = pd.read_csv(f'http://www.reades.com/{fn}', low_memory=False, encoding=\"ISO-8859-1\").set_index('EThOS_ID')\n",
    "    df.to_csv(os.path.join('data',fn), compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.tokens.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I'm confident about the output from UMAP in a _general_ sense, I'm much less certain about the default _distance_ measure (see [sklearn docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html)). I wonder if the metric should be `cosine`? [This article](https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa) appears to offer some help, but points on to a [longer discussion](https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions) where `manhattan` is argued to be a good representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMAP offers a very wide range of distance metrics:\n",
    "\n",
    "- Minkowski style metrics\n",
    "  - euclidean\n",
    "  - manhattan\n",
    "  - chebyshev\n",
    "  - minkowski\n",
    "- Miscellaneous spatial metrics\n",
    "  - canberra\n",
    "  - braycurtis\n",
    "  - haversine\n",
    "- Normalized spatial metrics\n",
    "  - mahalanobis\n",
    "  - wminkowski\n",
    "  - seuclidean\n",
    "- Angular and correlation metrics\n",
    "  - cosine\n",
    "  - correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes that there is a column that contains the \n",
    "# document embedding as an array/list that needs to be \n",
    "# extracted to a new data frame\n",
    "def x_from_df(df:pd.DataFrame, col:str='Embedding') -> pd.DataFrame:\n",
    "    cols = ['E'+str(x) for x in np.arange(0,len(df[col].iloc[0]))]\n",
    "    return pd.DataFrame(df[col].tolist(), columns=cols, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmeasure = 'euclidean'\n",
    "rdims    = 4 # r-dims == Reduced dimensionality\n",
    "print(f\"UMAP dimensionality reduction to {rdims} dimensions with '{dmeasure}' distance measure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Dimensionality\n",
    "\n",
    "Expect this to take **about 1 minute** (or **2.5 minutes on Google Collab**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "X = x_from_df(df, col='word_vec')\n",
    "    \n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=25,\n",
    "    min_dist=0.01,\n",
    "    n_components=rdims,\n",
    "    random_state=rs)\n",
    "    \n",
    "# Basically reduces our feature vectors for each thesis, down to n dimensions\n",
    "X_embedded = reducer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge on to Data\n",
    "\n",
    "This next block turns the output Numpy array into a data frame with one column for each reduced dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_dict = {}\n",
    "for i in range(0,X_embedded.shape[1]):\n",
    "    embedded_dict[f\"Dim {i+1}\"] = X_embedded[:,i] # D{dimension_num} (Dim 1...Dim n)\n",
    "\n",
    "# dfe == df embedded\n",
    "dfe = pd.DataFrame(embedded_dict, index=df.index)\n",
    "del(embedded_dict)\n",
    "\n",
    "dfe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the projection on to the main data frame so that we can easily explore the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected = df.join(dfe).sort_values(by=['ddc1','ddc2'])\n",
    "print(projected.columns.values)\n",
    "projected.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Embeddings by DDC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot these for reference, but don't save them as we spend more time tweaking the outputs in notebook 7 so we can do it for all outputs at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,2,figsize=(14,6))\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue='ddc1', s=5, alpha=0.1, ax=axs[0]);\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('DDC1 Group')\n",
    "axs[0].get_legend().set_title(\"\")\n",
    "\n",
    "sns.scatterplot(data=projected, x='Dim 1', y='Dim 2', hue='ddc2', s=5, alpha=0.1, ax=axs[1]);\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('DDC2 Group')\n",
    "axs[1].get_legend().set_title(\"\");\n",
    "#plt.savefig(os.path.join('data','DDC_Plot.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Intermediate Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected.reset_index().to_feather(os.path.join('data','ph-tutorial-data-final.feather'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this next stage to take **about 4 minutes** (or **8 minutes on Google Collab**). Using Google Collab I experienced a crash at this point which is why we have the intermediate saves above. You can reload the data from the feather even when the session ran out of RAM and died. If restarting from this point still produces Out-of-Memory errors than you may need to sample the data instead:\n",
    "```python\n",
    "projected = pd.read_feather(os.path.join('data','ph-tutorial-data-final.feather')).set_index('EThOS_ID').sample(frac=0.5)\n",
    "```\n",
    "When you perform the `join` later the unsampled records should fall out naturally though, obviously, your results will begin to differ substantially from the ones presented in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "Z = linkage(projected[[x for x in projected.columns if x.startswith('Dim ')]], method='ward', metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Intermediate Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Z, open(os.path.join('data','Z.pickle'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Matrix Output\n",
    "\n",
    "`Z` is a $(n-1)$ by 4 matrix. At the $i$-th iteration, clusters with indices $Z[i, 0]$ and $Z[i, 1]$ are combined to form cluster $n+i$. A cluster with an index less than $n$ corresponds to one of the $n$ original observations. The distance between clusters $Z[i, 0]$ and $Z[i, 1]$ is given by $Z[i, 2]$. The fourth value $Z[i, 3]$ represents the number of original observations in the newly formed cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "\n",
    "# Take the 1st, 10000th, '-2000th', and '-1st' observations\n",
    "for i in [0, 10000, -2000, -1]:\n",
    "    r = list(Z[i])\n",
    "    r.insert(0,(i if i >= 0 else len(Z)+i))\n",
    "    table.append(r)\n",
    "    table[-1][1] = int(table[-1][1])\n",
    "    table[-1][2] = int(table[-1][2])\n",
    "    table[-1][4] = int(table[-1][4])\n",
    "\n",
    "display(\n",
    "    tabulate(table, \n",
    "             headers=[\"Iteration\",\"$c_{i}$\",\"$c_{j}$\",\"$d_{ij}$\",\"$\\sum{c_{i},c_{j}}$\"], \n",
    "             floatfmt='0.3f', tablefmt='html'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Top _n_ Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cls = 100 # The number of last clusters to show in the dendogram\n",
    "\n",
    "plt.title(f'Hierarchical Clustering Dendrogram (truncated at {last_cls} clusters)')\n",
    "plt.xlabel('Sample Index (includes count of records in cluster)')\n",
    "plt.ylabel('Distance')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(20, 7)\n",
    "fig.set_dpi(300)\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode='lastp', # truncate dendrogram to the last p merged clusters\n",
    "    p=last_cls,            # and set a value for last p merged clusters\n",
    "    show_leaf_counts=True, # if parentheses then this is a count of observations, otherwise an id\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=False, # to get a distribution impression in truncated branches\n",
    ")\n",
    "#plt.savefig(os.path.join('data',f'Dendogram-{c.dmeasure}-{last_cls}.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette Scoring\n",
    "\n",
    "This process is slow and computationally intensive: it is calculating a silhouette score for every clustering option between `start` and `end` to give you a sense of how the silhouette score evolves with the number of clusters. A falling silhouette score is normal since, the smaller the number of observations in the cluster, the more you're likely to see some badly-bitted observations within a cluster... at least up until the point where you start having very small clusters indeed. What we're going to be looking for is the 'knee' where this process levels out.\n",
    "\n",
    "You're looking at **_over_ 10 seconds per clustering**, IIRC, so for 50 clusters that's **about 10 minutes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "start_cl = 2\n",
    "end_cl   = 50\n",
    "\n",
    "sil_scores = []\n",
    "\n",
    "print(\"Scoring cluster levels: \", end=\"\")\n",
    "\n",
    "X_embedded = projected[[x for x in projected if x.startswith('Dim ')]]\n",
    "\n",
    "for i in range(start_cl,end_cl):\n",
    "    print(\".\", end=\"\")\n",
    "    clusterings = fcluster(Z, i, criterion='maxclust')\n",
    "    \n",
    "    # Calculate silhouett average\n",
    "    sil_avg = silhouette_score(X_embedded, clusterings)\n",
    "    \n",
    "    # Append silhouette scores\n",
    "    sil_scores.append(sil_avg)\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plot\n",
    "\n",
    "Using the silhouette scores calculatee above we can now generate a scree plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(10,5));\n",
    "sns.lineplot(x=np.arange(start_cl,start_cl+len(sil_scores)), y=sil_scores, ax=ax)\n",
    "ax.set_title(\"\")\n",
    "ax.set_xlabel(\"Number of Clusters\")\n",
    "ax.set_ylabel(\"Average Silhouette Score\")\n",
    "\n",
    "decreasing = True\n",
    "last_sil   = 1.0\n",
    "selected   = 0\n",
    "\n",
    "for i in range(1,len(sil_scores)):\n",
    "    if sil_scores[i] < last_sil and decreasing:\n",
    "        #print(f\"Decreasing {i}: {sil_scores[i]:0.5f}\")\n",
    "        last_sil = sil_scores[i]\n",
    "    elif sil_scores[i] > last_sil and decreasing:\n",
    "        #print(f\"Increasing {i}: {sil_scores[i]:0.5f}\")\n",
    "        last_sil = sil_scores[i]\n",
    "        decreasing = False\n",
    "    elif sil_scores[i] > last_sil and not decreasing:\n",
    "        #print(f\"Increasing {i}: {sil_scores[i]:0.5f}\")\n",
    "        last_sil = sil_scores[i]\n",
    "        selected = i\n",
    "\n",
    "ax.vlines(x=selected+start_cl, ymin=np.array(sil_scores).min(), ymax=np.array(sil_scores).max(), color='red', linestyle='dotted')\n",
    "plt.text(x=selected+start_cl+1, y=np.array(sil_scores).max()/2, s=f'Suggest $c$={selected+start_cl}')\n",
    "\n",
    "plt.suptitle(f\"Scree Plot for Hierarchical Clustering\", fontsize=14);\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-Scree.png'), dpi=150)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knee Locator \n",
    "\n",
    "We can eyeball the scree plot, but a good sanity-check is to use the [kneed](https://pypi.org/project/kneed/) utility to automate the process. Depending on how your clusters score this may or may not be helpful: _e.g._ sometimes a relatively small deviation triggers the 'knee' when that is obviously only a 'blip'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KneeLocator(np.arange(3,3+len(sil_scores)), sil_scores, curve=\"convex\", direction=\"decreasing\")\n",
    "print(f'Suggest c={kn.knee}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to investigate the clustering results in greater detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters(src_df:pd.DataFrame, clusterings:np.ndarray, ddc_level:int=1):\n",
    "    \n",
    "    num_clusters = clusterings.max()\n",
    "    \n",
    "    tmp = pd.DataFrame({f'Cluster_{num_clusters}':clusterings}, index=src_df.index)\n",
    "    \n",
    "    joined_df = src_df.join(tmp, how='inner')\n",
    "    \n",
    "    labels = get_dominant_cat(joined_df, clusterings.max(), ddc_level)\n",
    "    joined_df[f'Cluster_Name_{num_clusters}'] = joined_df[f'Cluster_{num_clusters}'].apply(lambda x: labels[x])\n",
    "\n",
    "    return joined_df\n",
    "\n",
    "def plt_silhouette(src_df:pd.DataFrame, clusterings:np.ndarray) -> plt:\n",
    "    \n",
    "    num_clusters = clusterings.max()\n",
    "    sample_silhouette_values = silhouette_samples(src_df, clusterings)\n",
    "    \n",
    "    scale = cm.get_cmap(get_scale_nm(num_clusters)).colors\n",
    "        \n",
    "    fig, ax = plt.subplots(1,1,figsize=(10,7));\n",
    "    y_lower = 10\n",
    "    mx = clusterings.min()\n",
    "\n",
    "    for cl in range(1,clusterings.max()+1):\n",
    "\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them. \n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[clusterings==cl]\n",
    "        ith_cluster_silhouette_values.sort() # Note, returns None!\n",
    "        y_upper = y_lower + ith_cluster_silhouette_values.shape[0]\n",
    "\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=scale[cl],\n",
    "            edgecolor=scale[cl],\n",
    "            alpha=1.0,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        #ax.text(-0.05, y_lower + 0.5 * ith_cluster_silhouette_values.shape[0], str(c))\n",
    "        ax.annotate(f'Cluster {cl}',\n",
    "            xy=(np.min(sample_silhouette_values), y_lower + 0.5 * ith_cluster_silhouette_values.shape[0]), \n",
    "            textcoords='data', horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax.set_title(f\"Silhouette Plot for {num_clusters} Clusters\", fontsize=14);\n",
    "    ax.set_xlabel(\"Silhouette coefficient values\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "\n",
    "    return ax\n",
    "\n",
    "def get_dominant_cat(clustered_df:pd.DataFrame, num_clusters:int, ddc_level:int=1):\n",
    "    labels = {}\n",
    "    struct = {}\n",
    "\n",
    "    # First, work out the dominant group in each cluster\n",
    "    # and note that together with the cluster number --\n",
    "    # this gives us a dict with key==dominant group and \n",
    "    # then one or more cluster numbers from the output\n",
    "    # above.\n",
    "    for cl in range(1,num_clusters+1):\n",
    "    \n",
    "        # Identify the dominant 'domain' (i.e. group by\n",
    "        # DDC description) using the value counts result.\n",
    "        dom     = clustered_df[clustered_df[f'Cluster_{num_clusters}']==cl][f'ddc{ddc_level}'].value_counts().index[0]\n",
    "        print(f\"Cluster {cl} dominated by {dom} theses.\")\n",
    "    \n",
    "        if struct.get(dom) == None:\n",
    "            struct[dom] = []\n",
    "    \n",
    "        struct[dom].append(cl)\n",
    "\n",
    "    # Next, flip this around so that we create useful\n",
    "    # cluster labels for each cluster. Since we can have\n",
    "    # more than one cluster dominated by the same group\n",
    "    # we have to increment them (e.g. History 1, History 2)\n",
    "    for g in struct.keys():\n",
    "        if len(struct[g])==1:\n",
    "            labels[struct[g][0]]=g\n",
    "            #print(f'{g} maps to Cluster {struct[g][0]}')\n",
    "        else:\n",
    "            for s in range(0,len(struct[g])):\n",
    "                labels[struct[g][s]]=f'{g} {s+1}'\n",
    "                #print(f'{g} {s+1} maps to Cluster {struct[g][s]}')\n",
    "    return labels\n",
    "\n",
    "def get_scale_nm(num_clusters:int):\n",
    "    if num_clusters <= 10:\n",
    "        return 'tab10'\n",
    "    elif num_clusters <= 20:\n",
    "        return 'tab20'\n",
    "    else:\n",
    "        print(\"More than 20 clusters, this is hard to render meaningfully!\")\n",
    "        #cmap = mcolors.LinearSegmentedColormap.from_list(\"\", [\"indigo\",\"gold\"], gamma=0.5, N=num_clusters)\n",
    "        #cmap = mcolors.ListedColormap.from_list(\"\", [\"indigo\",\"gold\"], gamma=0.5, N=num_clusters)\n",
    "        cmap = cm.get_cmap('Spectral', num_clusters)\n",
    "        return cmap(np.linspace(0,1,cmap.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 2\n",
    "ddc_level = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Plot silhouette\n",
    "ax = plt_silhouette(projected[[x for x in projected.columns if x.startswith('Dim ')]], clusterings)\n",
    "f = ax.get_figure()\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-c{num_clusters}.png'), dpi=150)\n",
    "plt.show();\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Diagnostics\n",
    "print()\n",
    "\n",
    "# Classification report gives a (statistical) sense of power (TP/TN/FP/FN)\n",
    "print(classification_report(clustered_df[f'ddc{ddc_level}'], clustered_df[f'Cluster_Name_{num_clusters}']))\n",
    "\n",
    "# A confusion matrix is basically a cross-tab (without totals, which I think are nice to add)\n",
    "pd.crosstab(columns=clustered_df[f'Cluster_Name_{num_clusters}'], \n",
    "            index=clustered_df[f'ddc{ddc_level}'], \n",
    "            margins=True, margins_name='Total')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "ddc_level = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Plot silhouette\n",
    "ax = plt_silhouette(projected[[x for x in projected.columns if x.startswith('Dim ')]], clusterings)\n",
    "f = ax.get_figure()\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-c{num_clusters}.png'), dpi=150)\n",
    "plt.show();\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Diagnostics\n",
    "print()\n",
    "\n",
    "# Classification report gives a (statistical) sense of power (TP/TN/FP/FN)\n",
    "print(classification_report(clustered_df[f'ddc{ddc_level}'], clustered_df[f'Cluster_Name_{num_clusters}']))\n",
    "\n",
    "# A confusion matrix is basically a cross-tab (without totals, which I think are nice to add)\n",
    "pd.crosstab(columns=clustered_df[f'Cluster_Name_{num_clusters}'], \n",
    "            index=clustered_df[f'ddc{ddc_level}'], \n",
    "            margins=True, margins_name='Total')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected _n_ Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "ddc_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Plot silhouette\n",
    "ax = plt_silhouette(projected[[x for x in projected.columns if x.startswith('Dim ')]], clusterings)\n",
    "f = ax.get_figure()\n",
    "#plt.savefig(os.path.join('data',f'Silhouette-c{num_clusters}.png'), dpi=150)\n",
    "plt.show();\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Diagnostics\n",
    "print()\n",
    "\n",
    "# If you have a situation where multiple clusters have been detected within \n",
    "#Â a DDC (e.g. Biochemistry 1, 2, 3) then you need to combine these back into\n",
    "# Biochemsitry for the classification and crosstab or you'll get zero errors\n",
    "#Â because you couldn't have predicted Biochemistry 2 from Biochemsitry...\n",
    "sddc = clustered_df[f'ddc{ddc_level}']\n",
    "scl  = clustered_df[f'Cluster_Name_{num_clusters}']\n",
    "\n",
    "keys = sorted(scl.unique())\n",
    "vals = [re.sub(' \\d+$','',x) for x in keys]\n",
    "mapping = dict(zip(keys, vals))\n",
    "\n",
    "# Classification report gives a (statistical) sense of power (TP/TN/FP/FN)\n",
    "print(classification_report(sddc, scl.map(mapping), zero_division=1))\n",
    "\n",
    "# A confusion matrix is basically a cross-tab (without totals, which I think are nice to add)\n",
    "pd.crosstab(columns=scl.map(mapping), index=sddc, margins=True, margins_name='Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misclassifications\n",
    "\n",
    "Here we are trying to look in more detail at the PhDs that have (potentially!) been 'misclassified' by the experts--our clustering places them in a different group from the one specified by the DDC. Clearly, we'll have some false-positives in here as well, but the point is to examine the degree to which misclassification is both plausible and useful in terms of demonstrating the value of the NLP approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This makes it easier to read the thesis titles in the output\n",
    "pd.set_option('display.max_colwidth',150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = ['study','examine','analysis','system','use','design','model','data','within']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recluster\n",
    "\n",
    "Not needed if the `num_clusters` and `ddc_level` are the same as you ran above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4\n",
    "ddc_level    = 2\n",
    "\n",
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge with Intermediate Save\n",
    "\n",
    "We need the tokens again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_feather(os.path.join('data','ph-tutorial-data-embeddings.feather')).set_index('EThOS_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Data Frame (fdf)\n",
    "fdf = df.join(clustered_df, rsuffix='_dupe')\n",
    "fdf.drop(columns=[x for x in fdf.columns if x.endswith('_dupe')], inplace=True)\n",
    "fdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Misclassified Theses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach to misclassification works well for level 1 and level 2 of the DDC, but it gets a lot more complex when you're looking at level 3 because there are _so_ many different groups and misclassifications (e.g. Economics vs Financial Economics) that the results become much harder to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be based on whatever clustering above you ran *last* (ncls/ddc_level)\n",
    "# misc = mis-classified records\n",
    "misc = fdf[fdf[f'ddc{ddc_level}'] != fdf[f'Cluster_Name_{num_clusters}']]\n",
    "cols = ['Title',f'ddc{ddc_level}',f'Cluster_Name_{num_clusters}']\n",
    "misc.sample(5, random_state=rs)[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the distribution of 'misclassified' theses by DDC group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {misc.shape[0]:,} ({(misc.shape[0]/fdf.shape[0])*100:0.1f}%) 'misclassified' theses.\")\n",
    "print()\n",
    "misc.groupby(by=f'ddc{ddc_level}')[f'Cluster_Name_{num_clusters}'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "This approach is less technically sophisticated and robust than the one set out in Maarten Grootendorst's [CTFIDF](https://github.com/MaartenGr/cTFIDF/blob/master/ctfidf.py) module (as developed in [topic modelling with BERT](https://towardsdatascience.com/topic-modeling-with-bert-779f7db187e6) and [class-based TF/IDF](https://towardsdatascience.com/creating-a-class-based-tf-idf-with-scikit-learn-caea7b15b858)), but it saves having to install _another_ module and produces output that is easier to align with the needs of the WordCloud library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "tfidfs = {}\n",
    "\n",
    "vec = TfidfVectorizer(use_idf=True, ngram_range=(1,1), smooth_idf=True, stop_words=stopw)\n",
    "\n",
    "for d in fdf[f'ddc{ddc_level}'].unique():\n",
    "    \n",
    "    print(f\"Examining {d} DDC\")\n",
    "    tfidfs[d] = []\n",
    "    \n",
    "    # All records classified under this DDC\n",
    "    ddc_df = fdf[fdf[f'ddc{ddc_level}']==d].copy()\n",
    "    \n",
    "    # Those records that are part of this DDC\n",
    "    # but were clustered with *another* group\n",
    "    # going by the dominant class in that cluster.\n",
    "    sub_df = misc[misc[f'ddc{ddc_level}']==d].copy()\n",
    "    \n",
    "    print(f\"  ddc_df: {ddc_df.shape[0]:>7,}\")\n",
    "    print(f\"  sub_df: {sub_df.shape[0]:>7,}\")\n",
    "    print(f\"  remain: {ddc_df[~ddc_df.index.isin(misc.index)].shape[0]:>7,}\")\n",
    "    \n",
    "    print(f\"  {(sub_df.shape[0]/ddc_df.shape[0])*100:0.1f}% of {d} PhDs were clustered in other disciplines.\")\n",
    "    \n",
    "    # This removes the 'Earth Sc. 2', 'Earth Sc. 1' distinction for example.\n",
    "    # You would normally only encounter this working with DDC3.\n",
    "    sub_df.loc[:,'Cluster Name'] = sub_df[f'Cluster_Name_{num_clusters}'].str.replace(\"\\s\\d+$\",\"\",regex=True)\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    # And fit the corpus using IDF\n",
    "    corpus  = ddc_df.tokens.str.join(' ').fillna(' ').values \n",
    "    vec.fit(corpus)\n",
    "    \n",
    "    # One image per DDC Category\n",
    "    f,axes = plt.subplots(1, len(sub_df['Cluster Name'].unique()), figsize=(14,5))\n",
    "    \n",
    "    for i, cl in enumerate(sub_df['Cluster Name'].unique()):\n",
    "        \n",
    "        sub_cdf = sub_df[sub_df['Cluster Name']==cl]\n",
    "        print(f\"  PhDs classified as {cl} ({sub_cdf.shape[0]:,})\")\n",
    "        \n",
    "        tcorpus = vec.transform(sub_cdf.tokens.str.join(' ').fillna(' ').values)\n",
    "        \n",
    "        tfidf   = pd.DataFrame.from_records(tcorpus.toarray(), index=sub_cdf.index, columns=vec.get_feature_names_out())\n",
    "        tfterms = tfidf.T.sum(axis=1)\n",
    "        \n",
    "        tfidfs[d].append(\n",
    "            pd.DataFrame(\n",
    "              {f\"{cl} Term\":  tfterms.sort_values(ascending=False).index, \n",
    "               f\"{cl} Value\": tfterms.sort_values(ascending=False).values}\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        #print(tfterms.sort_values(ascending=False).head(5))\n",
    "        #print()\n",
    "        \n",
    "        Cloud = WordCloud(background_color=None, mode='RGBA', relative_scaling=0.5, font_path=fp)\n",
    "        \n",
    "        ax = axes.flatten()[i]\n",
    "        ax.set_title(f\"{d} clustered with {cl} ($n$={sub_cdf.shape[0]:,})\")\n",
    "        ax.imshow(Cloud.generate_from_frequencies(tfterms))\n",
    "        ax.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(os.path.join('images',f\"DDC_Cloud-c{num_clusters}-ddc{d}-tfidf.png\"), dpi=150)\n",
    "    plt.show()\n",
    "        \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15-Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 15\n",
    "ddc_level    = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clustering based on Z object\n",
    "clusterings  = fcluster(Z, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Label clusters and add to df\n",
    "clustered_df = label_clusters(projected, clusterings, ddc_level=ddc_level)\n",
    "\n",
    "# Load up intermediate save\n",
    "df = pd.read_feather(os.path.join('data','ph-tutorial-data-embeddings.feather')).set_index('EThOS_ID')\n",
    "\n",
    "# Join up with tokens\n",
    "fdf = df.join(clustered_df, rsuffix='_dupe')\n",
    "fdf.drop(columns=[x for x in fdf.columns if x.endswith('_dupe')], inplace=True)\n",
    "fdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls  = [x for x in fdf[f'Cluster_Name_{num_clusters}'].unique() if isinstance(x,str)] # Catch unexpected unlabelled result\n",
    "print(f\"Found clusters: {', '.join(sorted(cls))}.\")\n",
    "cols = 3\n",
    "rows = math.ceil(len(cls)/cols)\n",
    "\n",
    "vec = TfidfVectorizer(use_idf=True, ngram_range=(1,1), smooth_idf=True, stop_words=stopw)\n",
    "\n",
    "# We only need to calculate this once since \n",
    "# we're comparing to the full corpus\n",
    "idf_df  = fdf\n",
    "corpus  = idf_df.tokens.str.join(' ').fillna(' ').values # Convert tokens back to string\n",
    "vec.fit(corpus) # And fit the corpus using IDF\n",
    "\n",
    "f,axs = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n",
    "\n",
    "for i,v in enumerate(sorted(cls)):\n",
    "    \n",
    "    #Â Truncate cluster long name (cl.nm) just in case\n",
    "    if len(v) > 26:\n",
    "        cln = f\"{v[:26]}...\"\n",
    "    else:\n",
    "        cln = v\n",
    "    ax = axs.flatten()[i]\n",
    "    \n",
    "    print(f\"Cluster name is '{cln}'.\")\n",
    "    \n",
    "    # The TF is from the selected cluster\n",
    "    tf_df  = fdf[fdf[f'Cluster_Name_{num_clusters}']==v]\n",
    "    \n",
    "    # Share of corpus\n",
    "    corpus_share = f\"{(tf_df.shape[0]/idf_df.shape[0])*100:0.2f}\"\n",
    "    print(f\"  Cluster contains {tf_df.shape[0]:,} ({corpus_share}% of records)\")\n",
    "    \n",
    "    # Transform the selected cluster documents\n",
    "    tcorpus = vec.transform(tf_df.tokens.str.join(' ').fillna(' ').values)\n",
    "    \n",
    "    # And get the words back out in a format that\n",
    "    # we can use for a word cloud\n",
    "    tfidf   = pd.DataFrame.from_records(tcorpus.toarray(), index=tf_df.index, columns=vec.get_feature_names_out())\n",
    "    tfterms = tfidf.T.sum(axis=1)\n",
    "    \n",
    "    Cloud = WordCloud(background_color=None, mode='RGBA', \n",
    "                      width=int(ax.bbox.width), height=int(ax.bbox.height),\n",
    "                      relative_scaling=0.5, font_path=fp, \n",
    "                      max_words=50, random_state=rs)\n",
    "    \n",
    "    #print(f\"width={int(ax.bbox.width)}, height={int(ax.bbox.height)}\")\n",
    "    \n",
    "    ax.title.set_text(f\"{cln} ({corpus_share}% of docs)\")\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(Cloud.generate_from_frequencies(tfterms))\n",
    "    print(\"  Done.\")\n",
    "\n",
    "#plt.suptitle(f\"Distinctive Words for Clusters\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('data',f\"Word_Cloud-c{num_clusters}-tfidf.png\"), dpi=150)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
